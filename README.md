# JOAT - Just One AI Tool

A multi-model conversation system that intelligently routes user queries to specialized AI models based on task type, maintaining conversation context for each model. Now with **local Ollama models** and a **beautiful desktop GUI**!

## Features

- **Intelligent Task Classification**: Automatically classifies user queries into different task types
- **Multi-Model Routing**: Routes queries to the most appropriate AI model for each task
- **Conversation Context**: Maintains separate conversation histories for each model
- **Local AI Models**: Uses Ollama for cost-effective local AI processing
- **Beautiful Desktop GUI**: Modern chat interface inspired by leading chat apps
- **Command Line Interface**: Traditional CLI for power users
- **Easy Setup**: Simple installation and configuration

## ğŸ–¥ï¸ **Desktop GUI vs Command Line**

### **Desktop GUI (Recommended)**
```bash
# On any OS (macOS, Windows, Linux)
python gui_app.py

# Or on macOS/Linux
./start_gui.sh
```

**Features:**
- ğŸ¨ Beautiful modern interface
- ğŸ’¬ Real-time chat experience
- ğŸ“Š Live model status display
- ğŸ—‘ï¸ Easy conversation management
- âŒ¨ï¸ Enter to send, Shift+Enter for new line

### **Command Line Interface**
```bash
python app.py
```

**Features:**
- âš¡ Fast and lightweight
- ğŸ”§ Full control and debugging
- ğŸ“ Scriptable and automatable
- ğŸ–¥ï¸ Works on any terminal

## Supported Task Types

| Task Type | Model | Description | Size | Priority |
|-----------|-------|-------------|------|----------|
| `coding_generation` | **codellama** | Code generation, debugging, programming tasks | ~3.8GB | ğŸ”¥ High |
| `text_generation` | **llama3** | Creative writing, content generation | ~4.7GB | ğŸ”¥ High |
| `mathematical_reasoning` | **wizard-math** | Math problems, calculations, equations | ~4.1GB | ğŸ”¥ High |
| `commonsense_reasoning` | **phi3** | Logical reasoning, explanations, common sense | ~2.7GB | âš¡ Medium |
| `question_answering` | **mistral** | Factual questions, information retrieval | ~4.1GB | ğŸ”¥ High |
| `dialogue_systems` | **llama3** | General conversation, chat | ~4.7GB | ğŸ”¥ High |
| `summarization` | **mixtral** (optional) / **mistral** (default) | Text summarization, key point extraction | ~26GB / ~4.1GB | ğŸ’¡ Low |
| `sentiment_analysis` | **phi3** | Emotion analysis, sentiment detection | ~2.7GB | âš¡ Medium |
| `visual_question_answering` | **llava** | Image analysis, visual questions | ~4.5GB | âš¡ Medium |
| `video_question_answering` | **llama3** | Video analysis, motion understanding | ~4.7GB | ğŸ’¡ Low |

**Why This Model Selection?**

- **ğŸ”¥ High Priority**: Essential models for core functionality
- **âš¡ Medium Priority**: Specialized models for enhanced capabilities  
- **ğŸ’¡ Low Priority**: Advanced features for power users

Each model is carefully chosen for its specialization:
- **codellama**: Best-in-class code generation
- **wizard-math**: Specialized for mathematical reasoning
- **phi3**: Excellent for commonsense and sentiment analysis
- **mistral**: Strong general knowledge and Q&A
- **mixtral**: Advanced summarization capabilities
- **llava**: Visual understanding and image analysis

## ğŸš€ Quick Start

### 1. **Install Ollama**
```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/
```

### 2. **Start Ollama**
```bash
brew services start ollama  # macOS
# or
ollama serve               # Any platform
```

### 3. **Install Python Dependencies**
```bash
pip install -r requirements.txt
```

### 4. **Install AI Models** (Optional - will auto-download when needed)
```bash
python setup_ollama.py
```

### 5. **Launch the App**

**Desktop GUI (Recommended):**
```bash
python gui_app.py
# or on macOS/Linux
./start_gui.sh
```

**Command Line:**
```bash
python main.py
```

## ğŸ“¦ **Model Installation**

The app will automatically download models as needed, but you can pre-install them:

### **Comprehensive Setup (Recommended)**
```bash
# Install the best specialized models for each task
python setup_comprehensive_models.py
```

This script will:
- Show you which models are recommended for each task type
- Let you choose installation priority (High/Medium/Low)
- Calculate total download size
- Install models with progress tracking

### **Manual Installation**
```bash
# Install all recommended models
python setup_ollama.py

# Or install individually
ollama pull llama3
ollama pull codellama
ollama pull wizard-math
ollama pull phi3
ollama pull mistral
ollama pull mixtral
ollama pull llava
```

### **Installation Options**

**ğŸ”¥ High Priority (Essential - ~17GB total):**
- `codellama` - Code generation
- `llama3` - Text generation & dialogue
- `wizard-math` - Mathematical reasoning
- `mistral` - Question answering

**âš¡ Medium Priority (Enhanced - +12GB):**
- `phi3` - Commonsense & sentiment analysis
- `llava` - Visual question answering

**ğŸ’¡ Low Priority (Advanced - +30GB):**
- `mixtral` - Advanced summarization

## ğŸ¯ **Usage Examples**

### **Desktop GUI**
1. Launch: `python gui_app.py` (or `./start_gui.sh` on macOS/Linux)
2. Type your query in the input box
3. Press Enter or click Send
4. Watch the AI route your query to the best model
5. See which model was used in the response

### **Command Line**
```bash
python main.py

You: Write a Python function to calculate fibonacci numbers
ğŸ¤– [Response from codellama]

You: Solve the equation 2x + 5 = 13
ğŸ¤– [Response from wizard-math]

You: What is the capital of France?
ğŸ¤– [Response from llama3]
```

## ğŸ”§ **Configuration**

### **Models Mapping**
Edit `models_mapping.txt` to customize which models handle which tasks:

```
{coding_generation: codellama,
text_generation: llama3,
mathematical_reasoning: wizard-math,
...}
```

### **Adding New Models**
1. Install the model: `ollama pull your-model`
2. Add to `models_mapping.txt`
3. Restart the app

## ğŸ› ï¸ **Troubleshooting**

See the [Advanced Guide](docs/ADVANCED_GUIDE.md) for troubleshooting and performance tips.

## ğŸ“œ License

The JOAT source code is licensed under the **Apache License 2.0**. You can find the full license text in the [LICENSE](LICENSE) file.

### AI Model Licenses
The AI models used by this project have their own licenses. When you use JOAT, you are also subject to the license terms of the underlying models, which include:
- **Llama 3 Community License**: Applies to models like `llama3` and `codellama`.
- **Apache 2.0**: Applies to models like `mistral`.

It is your responsibility to review and comply with the terms of each model's license and its Acceptable Use Policy.

## ğŸ“ **File Structure**

```
joat/
â”œâ”€â”€ main.py              # CLI version
â”œâ”€â”€ app.py               # Core application logic
â”œâ”€â”€ gui_app.py           # The GUI application window
â”œâ”€â”€ start_gui.sh         # GUI launch script for macOS/Linux
â”œâ”€â”€ ollama_client.py     # Ollama API client
â”œâ”€â”€ setup_ollama.py      # Model setup script
â”œâ”€â”€ models_mapping.txt   # Task-to-model mapping
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ docs/                # Documentation files
â””â”€â”€ README.md            # This file
```

## ğŸ¨ **GUI Features**

- **Modern Design**: Clean, modern chat-like interface
- **Real-time Status**: Live Ollama and model status
- **Model Information**: Shows which models are available
- **Conversation History**: Maintains chat history
- **Keyboard Shortcuts**: Enter to send, Shift+Enter for new line
- **Responsive Layout**: Adapts to window size
- **Error Handling**: Graceful error messages

## ğŸ”„ **Conversation Management**

### **In GUI:**
- Click "ğŸ—‘ï¸ Clear History" to clear all conversations
- Each model maintains its own conversation context
- Automatic model switching based on task type

### **In CLI:**
- Type `clear` to clear history
- Type `history` to see conversation history
- Type `status` to check system status
- Type `quit` to exit

## ğŸ“ˆ **Performance Tips**

1. **SSD Storage**: Models load faster from SSD
2. **RAM**: 8GB+ recommended for smooth operation
3. **GPU**: Optional but speeds up inference (if supported by your hardware and Ollama)
4. **Model Selection**: Use smaller models for faster responses

## ğŸ¤ **Contributing**

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request


---

**Enjoy your local AI assistant! ğŸ¤–âœ¨** 
