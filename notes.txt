Models to consider:
deepseek-r1:8b (the default is 7b) as a mathematical_reasoning tool
llama3.1 as an update to llama3 
deepseek-r1:8b as update to question_answering
qwen3 as an update to dialogue dialogue_systems
deepseek-coder/qwen3 as an update to coding generation
MAth= 
Phi-3 Mini (3.8B parameters) - Dense decoder-only Transformer model fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) phi3
Phi-4 Mini Reasoning - 